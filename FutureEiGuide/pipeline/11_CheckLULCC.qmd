## Check LULCC {#11_CheckLULCC}

For checking the LULCC output integrity of the previous step,
an intensity analysis is performed.
As a previous measure of checking the LULCC output integrity,
a simple visual inspection of the output maps is recommended.
Subsequently, the intensity analysis regards the cumulative pixel-wise
change in land use and land cover (LULC) classes,
and computes the contingency table over a time series,
as a measure of change between each land use class.
These changes should be in a realistic range (e.g., between $0\%$ and $5\%$),
otherwise this can point to issues in the input data or the model itself.

:::{#cau-ia .callout-note}
This step automatically analyzes all LULCC scenarios.
But it is not integrated into the main execution script, as detailed later
in the [Running the pipeline](running.html) section.
:::

The configuration section for this step is as follows:

```{.yaml filename="src/config.yml" code-copy="false"}
# LULC check
CheckLULCC:
  InputDir: # keep empty to use FUTURE_EI_OUTPUT_DIR/LULCC_CH_OUTPUT_BASE_DIR
  OutputDir: # keep empty to use FUTURE_EI_OUTPUT_DIR/CHECK_LULCC_OUTPUT_DIR
  BaseName: LULCC_intensity_analysis # Can be used to distinguish different runs
  Parallel: True
  NWorkers: 0  # 0 means use all available cores
```

This step uses a conda environment with `raster~=3.6-26` aside further
R packages.
The automatic setup script
[`src/steps/11_CheckLULCC/11_CheckLULCC_setup.sh`](https://github.com/ethzplus/evoland-plus-HPC/tree/main/src/steps/11_CheckLULCC/11_CheckLULCC_setup.sh)
needs to be executed to set up the conda environment.
It sets up a conda environment `check_lulc` with the packages found in
`11_checklulcc_env.yml`.

Running the intensity analyis is as easy as [`sbatch`ing](https://slurm.schedmd.com/sbatch.html)
the job script
[`slurm_job.sh`](https://github.com/ethzplus/evoland-plus-HPC/tree/main/src/steps/11_CheckLULCC/slurm_job.sh).

```{.bash}
sbatch src/steps/11_CheckLULCC/slurm_job.sh
```

The `sbatch` command submits the job to the HPC scheduler with the running
options specified in the header of the job script.

```{.bash filename="src/steps/11_CheckLULCC/slurm_job.sh (lines 1-10)"}
#!/bin/bash
#SBATCH --job-name="11_check_lulcc"
#SBATCH -n 1                  # Number of cores requested
#SBATCH --cpus-per-task=25    # Number of CPUs per task
#SBATCH --time=4:00:00        # Runtime
#SBATCH --mem-per-cpu=4G      # Memory per cpu in GB (see also --mem)
#SBATCH --tmp=2G              # https://scicomp.ethz.ch/wiki/Using_local_scratch
#SBATCH --output="logs/11_check_lulcc-%j.out"
#SBATCH --error="logs/11_check_lulcc-%j.err"
#SBATCH --mail-type=NONE       # Mail events (NONE, BEGIN, END, FAIL, ALL)
```

Change these settings according to your needs and the available resources.
Monitor the logs in the `logs` directory to check the progress of the job.
If you want to specify more options,
refer to the [SLURM documentation](https://slurm.schedmd.com/sbatch.html#SECTION_OPTIONS)
or your local HPC documentation.
