[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "evoland+ HPC Guide",
    "section": "",
    "text": "evoland+ HPC Guide",
    "crumbs": [
      "evoland+ HPC Guide"
    ]
  },
  {
    "objectID": "index.html#statement-of-need",
    "href": "index.html#statement-of-need",
    "title": "evoland+ HPC Guide",
    "section": "Statement of need",
    "text": "Statement of need\nevoland-plus HPC was devised in order to perform simulations of alternative scenarios (Mayer, Rabe, and Grêt-Regamey 2023) that depict the development of the framing conditions for the establishment of a functioning Ecological Infrastructure in Switzerland as specified in the Action Plan for the Swiss Biodiversity Strategy.",
    "crumbs": [
      "evoland+ HPC Guide"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "evoland+ HPC Guide",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe development of evoland-plus HPC and it’s parent models was funded under the ValPar.CH project, funded by the Federal Office for the Environment (FOEN).\nN-SDM was developed within the Ecospatial Ecology Group (Ecospat) at the University of Lausanne\nLULCC-CH was developed within the chair of Planning of Landscape and Urban Systems (PLUS) at ETH Zurich\nThe models of NCPs were developed within the EnviroSPACE Laboratory at the University of Geneva.",
    "crumbs": [
      "evoland+ HPC Guide"
    ]
  },
  {
    "objectID": "index.html#sec-citation",
    "href": "index.html#sec-citation",
    "title": "evoland+ HPC Guide",
    "section": "How to cite",
    "text": "How to cite\nWhen using the evoland-plus HPC pipeline, please cite the following paper:\n\n\n\n\n\n\nBibTex\n\n\n\n@unpublished{black2025,\n    title = {Identifying robust conservation strategies to secure ecosystem service provision under uncertainties},\n    author = {Black, Benjamin and Adde, Antoine and {Külling}, Nathan and {Büth}, Carlson and Kurmann, Manuel and Lehmann, Anthony and Altermatt, Florian and Guisan, Antoine and Gret-Regamey, Adrienne},\n    year = {2025},\n    date = {2025},\n    note = {In Review in Global Environmental Change}\n}\n}\n\n\n\n\n\n\n\n\nAttribution\n\n\n\nBlack, B., Adde, A., Külling, N., Büth, C., Kurmann, M., Lehmann, A., Altermatt, F., Guisan, A., & Gret-Regamey, A. (2025). Identifying robust conservation strategies to secure ecosystem service provision under uncertainties. In review in Global Environmental Change.\n\n\nDownload BibTeX File",
    "crumbs": [
      "evoland+ HPC Guide"
    ]
  },
  {
    "objectID": "index.html#get-started-with-evoland-plus-hpc",
    "href": "index.html#get-started-with-evoland-plus-hpc",
    "title": "evoland+ HPC Guide",
    "section": "Get started with evoland-plus HPC",
    "text": "Get started with evoland-plus HPC",
    "crumbs": [
      "evoland+ HPC Guide"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Details on the background on the evoland+ HPC pipeline\n\nMotivation\nWhy\nClimate Scenarios\n\nmany → high computational demand\n\nCite Mayer, Rabe, and Grêt-Regamey (2023), Black et al. (2023), Adde et al. (2023)\nRelated work\n\nStructure of the next sections:\n\nThe Introduction you read\nStructure gives an overview of the pipeline\nPipeline describes the setup and steps in detail\nSummary concludes the guide\nReferences list all cited works\n\n\n\n\n\nAdde, Antoine, Pierre-Louis Rey, Philipp Brun, Nathan Külling, Fabian Fopp, Florian Altermatt, Olivier Broennimann, et al. 2023. “N-SDM : A High-Performance Computing Pipeline for Nested Species Distribution Modelling.” Ecography 2023 (6): e06540. https://doi.org/10.1111/ecog.06540.\n\n\nBlack, Benjamin, Maarten J. Van Strien, Antoine Adde, and Adrienne Grêt-Regamey. 2023. “Re-Considering the Status Quo: Improving Calibration of Land Use Change Models Through Validation of Transition Potential Predictions.” Environmental Modelling & Software 159 (January): 105574. https://doi.org/10.1016/j.envsoft.2022.105574.\n\n\nMayer, Paula, Sven-Erik Rabe, and Adrienne Grêt-Regamey. 2023. “Operationalizing the Nature Futures Framework for Ecological Infrastructure.” Sustainability Science, July. https://doi.org/10.1007/s11625-023-01380-7.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "structure.html",
    "href": "structure.html",
    "title": "2  Structure",
    "section": "",
    "text": "2.1 LULCC: Land Use Simulation\nFor the related research project Black et al. (2025), we couple three main steps: Land Use Simulation, combined NCP Estimations and Species Distribution Modeling. These are shown bold in the Figure 2.1. evoland-plus HPC covers the LULC Simulation and steps dependent on its output, namely the NCP estimations, intensity analysis and focal window preparation. These steps, shown in blue, are unified to be automated for command line interface (CLI) and high performance computing (HPC) compatibility, to be scalable and reproducible.\nFirst, we give an overview of each step in the pipeline. In contrast, the Pipeline section details and documents the individual steps and code used, in terms of input, output and execution.\nThe land use land cover change (LULCC) model for Switzerland (Black et al. 2023; Black et al. 2024) is the first step in the evoland-plus HPC pipeline. For given climate scenarios, it simulates land use changes in Switzerland until a given future year (e.g., 2060), based on historical data and future projections. The generated land use maps are then used as input for the following steps.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Structure</span>"
    ]
  },
  {
    "objectID": "structure.html#ncp-natures-contributions-to-people",
    "href": "structure.html#ncp-natures-contributions-to-people",
    "title": "2  Structure",
    "section": "2.2 NCP: Nature’s Contributions to People",
    "text": "2.2 NCP: Nature’s Contributions to People\nA range of NCP are estimated from the land use maps. Some NCP use further data for the estimation, e.g., precipitation and temperature projections.\nThe code is based on Külling et al. (2024), but has been adapted for automation and HPC compatibility.\n\n\n\nTable 2.1: Nature’s Contributions to People (NCP) estimated from the LULCC model.\n\n\n\n\n\nNCP\nName\nIndicator\n\n\n\n\nCAR\nRegulation of climate\nCarbon stored in biomass and soil\n\n\nFF\nFood and feed\nCrop production potential (ecocrop)\n\n\nHAB\nHabitat creation and maintenance\nHabitat quality index\n\n\nNDR\nNutrient Delivery Ratio\nAnnual nutrient retention by vegetation\n\n\nPOL\nPollination and dispersal of seeds\nHabitat abundance for pollinators\n\n\nREC\nRecreation potential\nRecreation potential (RP) provided by ecosystems\n\n\nSDR\nFormation, protection and decontamination of soils\nErosion control by sediment retention\n\n\nWY\nRegulation of freshwater quantity, location and timing\nAnnual water yield",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Structure</span>"
    ]
  },
  {
    "objectID": "structure.html#intensity-analysis",
    "href": "structure.html#intensity-analysis",
    "title": "2  Structure",
    "section": "2.3 Intensity Analysis",
    "text": "2.3 Intensity Analysis\nThe Intensity Analysis (IA) is based on x, and serves as",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Structure</span>"
    ]
  },
  {
    "objectID": "structure.html#focal-lulc-preparation",
    "href": "structure.html#focal-lulc-preparation",
    "title": "2  Structure",
    "section": "2.4 Focal LULC Preparation",
    "text": "2.4 Focal LULC Preparation\nExplain why These focal layers are used as inputs for X and Y, and act as NCP, specifically Z.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Structure</span>"
    ]
  },
  {
    "objectID": "structure.html#n-sdm-nested-species-distribution-modelling",
    "href": "structure.html#n-sdm-nested-species-distribution-modelling",
    "title": "2  Structure",
    "section": "2.5 N-SDM: Nested Species Distribution Modelling",
    "text": "2.5 N-SDM: Nested Species Distribution Modelling\nThe nested species distribution modelling (N-SDM) (Adde et al. 2023) step simulates the distribution of species in Switzerland. Species occurrence used to fit models and covariate data are selected, both at different spatial scales, to predict the species distribution. This part is not integrated into the evoland-plus HPC pipeline, and needs to be consulted separately.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Structure</span>"
    ]
  },
  {
    "objectID": "structure.html#species-maps-aggregation",
    "href": "structure.html#species-maps-aggregation",
    "title": "2  Structure",
    "section": "2.6 Species Maps Aggregation",
    "text": "2.6 Species Maps Aggregation\nThe species maps are then aggregated to …",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Structure</span>"
    ]
  },
  {
    "objectID": "structure.html#clustering",
    "href": "structure.html#clustering",
    "title": "2  Structure",
    "section": "2.7 Clustering",
    "text": "2.7 Clustering\nTo analyze the scenarios, the species maps are clustered to … The code for this step can be found in the separate repository reponame. These scripts are adapted to the case of Switzerland and the evoland-plus HPC project. …\n\n\n\n\nAdde, Antoine, Pierre-Louis Rey, Philipp Brun, Nathan Külling, Fabian Fopp, Florian Altermatt, Olivier Broennimann, et al. 2023. “N-SDM : A High-Performance Computing Pipeline for Nested Species Distribution Modelling.” Ecography 2023 (6): e06540. https://doi.org/10.1111/ecog.06540.\n\n\nBlack, Benjamin, Antoine Adde, Daniel Farinotti, Antoine Guisan, Nathan Külling, Manuel Kurmann, Caroline Martin, et al. 2024. “Broadening the Horizon in Land Use Change Modelling: Normative Scenarios for Nature Positive Futures in Switzerland.” Regional Environmental Change 24 (3). https://doi.org/10.1007/s10113-024-02261-0.\n\n\nBlack, Benjamin, Antoine Adde, Nathan Külling, Carlson Büth, Manuel Kurmann, Anthony Lehmann, Florian Altermatt, Antoine Guisan, and Adrienne Gret-Regamey. 2025. “Identifying Robust Conservation Strategies to Secure Ecosystem Service Provision Under Uncertainties.”\n\n\nBlack, Benjamin, Maarten J. Van Strien, Antoine Adde, and Adrienne Grêt-Regamey. 2023. “Re-Considering the Status Quo: Improving Calibration of Land Use Change Models Through Validation of Transition Potential Predictions.” Environmental Modelling & Software 159 (January): 105574. https://doi.org/10.1016/j.envsoft.2022.105574.\n\n\nKülling, Nathan, Antoine Adde, Audrey Lambiel, Sergio Wicki, Antoine Guisan, Adrienne Grêt-Regamey, and Anthony Lehmann. 2024. “Nature’s Contributions to People and Biodiversity Mapping in Switzerland: Spatial Patterns and Environmental Drivers.” Ecological Indicators 163: 112079. https://doi.org/10.1016/j.ecolind.2024.112079.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Structure</span>"
    ]
  },
  {
    "objectID": "pipeline/pipeline.html",
    "href": "pipeline/pipeline.html",
    "title": "3  Setup and Usage",
    "section": "",
    "text": "3.1 Setup\nThe evoland-plus HPC pipeline consists out of various scripts that can be found in the src directory. For each included step of Figure 2.1, there is a subdirectory in src/steps.\nThis part is structured as follows:\nThe task of the evoland-plus HPC pipeline is to streamline the process, so that varying the climate scenarios and other parameters can be carried out efficiently. Introducing parallelization through SLURM batch jobs, and adding HPC compatibility, are the main tasks of the pipeline. Meanwhile, the pipeline keeps track of the intermediate results, a centralized configuration file, and the execution of each step. Details on the individual steps are given in the following sections.\nBefore you set up the evoland-plus HPC pipeline, you should make sure to satisfy a few requirements. This section will go over hardware and software requirements, and then guide you through the evoland-plus HPC repository setup. The following pages guide through the details of each step in the pipeline, before concluding with the execution of the pipeline.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setup and Usage</span>"
    ]
  },
  {
    "objectID": "pipeline/pipeline.html#setup",
    "href": "pipeline/pipeline.html#setup",
    "title": "3  Setup and Usage",
    "section": "",
    "text": "3.1.1 Requirements\nWe are using a Linux cluster with SLURM as scheduler. If your cluster uses a different scheduler, you can see if it is compatible with the SLURM syntax, or you can adapt the scripts to your scheduler.\n\n\n\n\n\n\nNote 3.1: What is a Linux cluster?\n\n\n\nAs this pipeline is specifically designed to simulate a large number of scenarios, it has been optimized for high-performance computing (HPC) environments. If you only need to run a few scenarios, it might be easier to run the steps manually. Otherwise, you do need to have access to a Linux cluster with SLURM. Feel free to reach out to a technically savvy colleague or your local HPC support for help.\n\n\n\n3.1.1.1 Hardware\nThe minimum memory and CPU requirements cannot generally be stated, as they depend on the area of interest, input data, and the number of scenarios. A viable starting point for a country with the size of Switzerland, using a resolution of 100 m, is 16 GB of memory and 4 CPUs. This is the case for a few scenarios and no parallelization within the steps. Scaling up to around 1000 scenarios, we suggest at least 128 GB of memory and 16 CPUs, to achieve a viable runtime. As this is an estimate, it is essential to monitor runtime before scaling up.\n\n\n3.1.1.2 Software\nAdditionally, you need to install the following software:\n\n3.1.1.2.1 Micromamba/Conda\nFor some pipeline steps, we use conda environments. Conda is a package manager that helps you manage dependencies in isolated environments. We recommend using micromamba, which does the same job as Conda, but resolves dependencies much faster, with the flexibility of miniconda (CLI of Conda). Find the installation instructions for Micromamba here. We have added compatibility for micromamba, mamba and conda, in this order of preference, but only tested with micromamba1.\nWe have chosen conda-forge as the default channel for the conda environments, as it is a single source for our R, Python, and lower-level dependencies (e.g., gdal, proj). This is independent of the modules and applications provided by the HPC environment.\n\n\n3.1.1.2.2 Apptainer\nRunning containerized applications on HPCs can be challenging. To simplify the process, we use the Apptainer (formerly Singularity) container runtime. Make sure your HPC environment supports Apptainer, and that you have the necessary permissions to run containers. If this is not the case, contact your HPC support team for help.\n\n\n3.1.1.2.3 Docker\nBuilding the LULCC container requires Docker2 before converting it to the Apptainer format. The lulcc container uses the dinamica-ego-docker container (version 7.5).\nThis step can be done on a local machine, and will be explained in the LULCC step.\n\n\n3.1.1.2.4 Dinamica EGO\nDinamica EGO is an environmental modeling platform used in the LULCC step. It is available on the project website. But as aforementioned, it will be used from the LULCC docker image, as it is only integrated from the command line interface (CLI), not with the usual graphical user interface (GUI).\n\n\n3.1.1.2.5 Yaml Parser yq\nFor the bash scripts, we use yq to parse the yaml configuration file. yq needs to be available in the PATH variable of the shell. To install the latest version3, run the following command:\nbin_dir=/usr/bin &&\\\nwget https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -O $bin_dir/yq\nchmod +x $bin_dir/yq\nOther installation options and binaries can be found on the repository’s README. To make yq available in the PATH variable, make sure the $bin_dir is in the PATH variable. To check the parser is installed correctly, run yq --version in the shell.\n\n\n3.1.1.2.6 LULCC Repository\nThe version used for evoland-plus HPC is a reduced version of the original model, adapted for containerized execution on HPCs, and can be found on the hpc branch of the repository. Clone the repository to the HPC using git or download the repository as a zip. If you have never used git before, search online for a guide on how to clone a repository.\n\n\n\n\n3.1.2 evoland-plus HPC Repository\nAfter you have set up the requirements, you can clone the evoland-plus HPC repository. This repository contains the pipeline and all necessary scripts to run it.\nBefore you start the pipeline, you need to configure the pipeline. These settings are centralized in the config.yml file. There are only a few mandatory changes we will highlight, but you can find more settings with descriptive names in the file.\n\n\nsrc/config.yml\n\n# Bash variables\nbash_variables:\n  FUTURE_EI_CONFIG_FILE: ~/evoland-plus HPC/src/config.yml\n  FUTURE_EI_OUTPUT_DIR: ~/evoland-plus HPC-Output\n  ...\n  # LULCC HPC version\n  LULCC_CH_HPC_DIR: ~/LULCC-CH\n  ...\n  # Overwrites $TMPDIR if not set by the system. $TMPDIR is used by Dinamica EGO.\n  # and conda/libmamba\n  ALTERNATIVE_TMPDIR: /scratch\n...\n\nFor each script, src/bash_common.sh is sourced to set the environment variables. First, FUTURE_EI_CONFIG_FILE needs to be set to the absolute path of this configuration file. FUTURE_EI_OUTPUT_DIR is the directory where the outputs of the pipeline will be stored. As the pipeline needs a multiple more temporary space than the output itself, having a fast and large temporary directory is crucial. If the HPC does not set the $TMPDIR variable, you can set it to a different directory using ALTERNATIVE_TMPDIR. This will be used in the LULCC and NCP steps for temporary files. Finally, LULCC_CH_HPC_DIR is the directory where the LULCC repository is stored, which was cloned in the previous step.\n\n\nsrc/config.yml\n\n# Focal LULC\nFocalLULCC:\n  ...\n\n# LULC check\nCheckLULCC:\n  ...\n\nTo mention the FocalLULCC and CheckLULCC sections, these are settings dedicated to separate steps in the pipeline and are specifically loaded in the respective scripts. We will touch on these settings in the respective steps. To see the current settings (and test yq), print the contents of config.yml as idiomatic YAML to stdout:\nyq -P -oy src/config.yml\nAs a last general note, make sure to set the permissions of the scripts to executable. To make all bash scripts in the source executable, give them the permission as follows:\n# possibly activate globstar: shopt -s globstar\nchmod +x src/**/*.sh\nThe next sections will guide you through the setup of each step in the pipeline.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setup and Usage</span>"
    ]
  },
  {
    "objectID": "pipeline/pipeline.html#steps",
    "href": "pipeline/pipeline.html#steps",
    "title": "3  Setup and Usage",
    "section": "3.2 Steps",
    "text": "3.2 Steps",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setup and Usage</span>"
    ]
  },
  {
    "objectID": "pipeline/pipeline.html#10_LULCC",
    "href": "pipeline/pipeline.html#10_LULCC",
    "title": "3  Setup and Usage",
    "section": "3.3 Land Use Land Cover Change",
    "text": "3.3 Land Use Land Cover Change\nLULCC is a Dinamica EGO (Leite-Filho et al. 2020) model, and makes use of the R (R Core Team 2022) ecosystem, including packages from the Comprehensive R Archive Network (CRAN). You can find the LULCC model, as well as an adapted version for use with evoland-plus HPC, in the LULCC repository (Black 2024), as mentioned in the setup section.\n\n\n\n\n\n\nCaution 3.2: Used Versions in this step\n\n\n\n\nCovered by LULCC docker 0.3.0:\n\nDinamica EGO: 7.5\nR: 4.3.2\n\nraster: 3.6-26\nFurther R packages4\n\n\n\n\n\nLULCC needs a variety of inputs. These are set via environment variables in the src/config.yml file — the assumption being that the src directory contains project-specific code, and hence also setup details. Here is an excerpt of the file:\n\n\nsrc/config.yml\n\n# Bash variables\nbash_variables:\n  ...\n  # Model Variables - from LULCC_CH_HPC root\n  LULCC_M_CLASS_AGG: Tools/LULC_class_aggregation.xlsx\n  LULCC_M_SPEC: Tools/Model_specs.csv\n  LULCC_M_PARAM_GRID: Tools/param-grid.xlsx\n  LULCC_M_PRED_TABLE: Tools/Predictor_table.xlsx\n  LULCC_M_REF_GRID: Data/Ref_grid.tif\n  LULCC_M_CAL_PARAM_DIR: Data/Allocation_parameters/Calibration\n  LULCC_M_SIM_PARAM_DIR: Data/Allocation_parameters/Simulation\n  LULCC_M_RATE_TABLE_DIR: Data/Transition_tables/prepared_trans_tables\n  LULCC_M_SIM_CONTROL_TABLE: ~/LULCC-CH/Tools/Simulation_control.csv\n  LULCC_M_SPAT_INTS_TABLE: Tools/Spatial_interventions.csv\n  LULCC_M_EI_INTS_TABLE: Tools/EI_interventions.csv\n  LULCC_M_SCENARIO_SPEC: Tools/Scenario_specs.csv\n  LULCC_M_EI_LAYER_DIR: Data/EI_intervention_layers\n  LULCC_M_REMOVE_PRED_PROB_MAPS: True # remove prediction probability maps after\n  # simulation if 1, True or TRUE\n\nA relevant parameter to change is the LULCC_M_SIM_CONTROL_TABLE variable. This is the only path that is absolute, and it should point to the Simulation_control.csv file. All further paths are relative to the LULCC repository root: the files under Tools are configuration files, while the Data directory contains input and working data. For information on the further variables, see the LULCC repository and paper (Black 2024).\n\n3.3.1 Simulation Control Table\nSimulation_control.csv is a table that controls the scenarios to be simulated, including the data described in Table 3.1. This format extends the original format from the LULCC model.\n\n\n~/LULCC-CH/Tools/Simulation_control.csv\n\nSimulation_num.,Scenario_ID.string,Simulation_ID.string,Model_mode.string,Scenario_start.real,Scenario_end.real,Step_length.real,Parallel_TPC.string,Pop_scenario.string,Econ_scenario.string,Climate_scenario.string,Spatial_interventions.string,EI_interventions.string,Deterministic_trans.string,Completed.string,EI_ID.string\n1,BAU,1,Simulation,2020,2060,5,N,Ref,Ref_Central,rcp45,Y,Y,Y,N,1\n217,EINAT,217,Simulation,2020,2060,5,N,Low,Ecolo_Urban,rcp26,Y,Y,Y,N,217\n433,EICUL,433,Simulation,2020,2060,5,N,Ref,Ecolo_Central,rcp26,Y,Y,Y,N,433\n649,EISOC,649,Simulation,2020,2060,5,N,Ref,Combined_Urban,rcp45,Y,Y,Y,N,649\n865,BAU,865,Simulation,2020,2060,5,N,Ref,Ref_Central,rcp85,Y,Y,Y,N,1\n\nEach colum describes one scenario to be simulated. This table controls which data is used to simulate the land use changes.\n\n\n\nTable 3.1: Description of the columns in the Simulation_control.csv file.\n\n\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nSimulation_num.\nThe number of the simulation.\n\n\nScenario_ID.string\nThe scenario ID.\n\n\nSimulation_ID.string\nThe simulation ID.\n\n\nModel_mode.string\nThe model mode.\n\n\nScenario_start.real\nThe start year of the scenario.\n\n\nScenario_end.real\nThe end year of the scenario.\n\n\nStep_length.real\nThe length of the steps.\n\n\nParallel_TPC.string\nWhether the simulation is parallelized.\n\n\nPop_scenario.string\nThe population scenario.\n\n\nEcon_scenario.string\nThe economic scenario.\n\n\nClimate_scenario.string\nThe climate scenario (e.g., rcp45, rcp26, rcp85).\n\n\nSpatial_interventions.string\nWhether spatial interventions are used.\n\n\nEI_interventions.string\nWhether EI interventions are used\n\n\nDeterministic_trans.string\nWhether deterministic transitions are used.\n\n\nCompleted.string\nWhether the simulation is completed.\n\n\nEI_ID.string\nThe EI ID.\n\n\n\n\n\n\n\n\n3.3.2 Container Setup\nFor a platform independent execution of Dinamica EGO, we created a dinamica-ego-docker container container. This way, the glibc version is fixed, and the container can be used system independently5. This one is used in the LULCC docker container. Our Dockerfile src/steps/10_LULCC/Dockerfile then adds the necessary R packages for LULCC to the container. The Apptainer Definition File src/steps/10_LULCC/lulcc.def bootstraps the docker container, mounts the LULCC_CH_HPC_DIR to the /model directory (it is not shipped within the container), and translates the entry point to the Apptainer format. This includes adding the necessary environment variables, connecting the Simulation Control Table, pointing Dinamica EGO to the correct R binary, among other details found in the Definition File. Figure 3.1 summarizes the levels of wrapping.\n\n\n\n\n\n\nflowchart LR\n    Dinamica([Dinamica EGO]) --&gt; Docker(dinamica-ego-docker)\n    Docker --&gt; LULCC(LULCC docker)\n    LULCC --&gt; Apptainer[Apptainer container]\n\n    style Dinamica color:#2780e3, fill:#e9f2fc, stroke:#000000\n    style Docker color:#0e7895, fill:#cbf4ff, stroke:#000000\n    style LULCC color:#0e7895, fill:#cbf4ff, stroke:#000000\n    style Apptainer color:#07946e, fill:#def9f2, stroke:#000000\n\n\n\n\nFigure 3.1: Visualization on how Dinamica EGO is wrapped until it can be used in Apptainer on the HPC.\n\n\n\n\n\nTo load the LULCC docker onto your system, it can be automatically installed or built using the src/steps/10_LULCC/docker_setup.sh script, which uses variables from the src/config.yml. If you have docker installed, the setup script guides you through the building, pushing, or pulling of the LULCC docker container. This step can be done on a local machine. Consecutively, when having apptainer installed, the LULCC docker can be converted to an Apptainer container. On the HPC, this latter step suffices if you use the pre-configured LULCC_DOCKER_REPO, unless you want to rebuild the container. The decisive line in the script is:\n\n\nsrc/steps/10_LULCC/docker_setup.sh (lines 84ff)\n\napptainer build \\\n      --build-arg \"namespace=$namespace\" --build-arg \"repo=$repo\" \\\n      --build-arg \"version=$version\" \\\n      \"$APPTAINER_CONTAINERDIR/${repo}_${version}.sif\" \"$SCRIPT_DIR/lulcc.def\"\n\nDepending on your system, you might want to reconfigure the Apptainer variables:\n\n\nsrc/config.yml\n\n# Bash variables\nbash_variables:\n  ...\n  # Apptainer variables for the apptainer container\n  APPTAINER_CONTAINERDIR: ~/apptainer_containers\n  APPTAINER_CACHEDIR: /scratch/apptainer_cache\n\nAPPTAINER_CONTAINERDIR is used to store the Apptainer containers, and APPTAINER_CACHEDIR is used when building them. If your HPC does not have a /scratch directory, you might want to change it to another temporary directory.\nAfter all previous steps are completed, you can test the LULCC model with some test scenarios in the simulation control table. src/steps/10_LULCC/slurm_job.sh submits. Before the full, parallelized simulation can be started, read the following sections.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setup and Usage</span>"
    ]
  },
  {
    "objectID": "pipeline/pipeline.html#11_CheckLULCC",
    "href": "pipeline/pipeline.html#11_CheckLULCC",
    "title": "3  Setup and Usage",
    "section": "3.4 Check LULCC",
    "text": "3.4 Check LULCC\nFor checking the LULCC output integrity of the previous step, an intensity analysis is performed. As a previous measure of checking the LULCC output integrity, a simple visual inspection of the output maps is recommended. Subsequently, the intensity analysis regards the cumulative pixel-wise change in land use and land cover (LULC) classes, and computes the contingency table over a time series, as a measure of change between each land use class. These changes should be in a realistic range (e.g., between \\(0\\%\\) and \\(5\\%\\)), otherwise this can point to issues in the input data or the model itself.\n\n\n\n\n\n\nNote 3.3\n\n\n\nThis step automatically analyzes all LULCC scenarios. But it is not integrated into the main execution script, as detailed later in the Running the pipeline section.\n\n\nThe configuration section for this step is as follows:\n\n\nsrc/config.yml\n\n# LULC check\nCheckLULCC:\n  InputDir: # keep empty to use FUTURE_EI_OUTPUT_DIR/LULCC_CH_OUTPUT_BASE_DIR\n  OutputDir: # keep empty to use FUTURE_EI_OUTPUT_DIR/CHECK_LULCC_OUTPUT_DIR\n  BaseName: LULCC_intensity_analysis # Can be used to distinguish different runs\n  Parallel: True\n  NWorkers: 0  # 0 means use all available cores\n\nThis step uses a conda environment with raster~=3.6-26 aside further R packages. The automatic setup script src/steps/11_CheckLULCC/11_CheckLULCC_setup.sh needs to be executed to set up the conda environment. It sets up a conda environment check_lulc with the packages found in 11_checklulcc_env.yml.\nRunning the intensity analyis is as easy as sbatching the job script slurm_job.sh.\nsbatch src/steps/11_CheckLULCC/slurm_job.sh\nThe sbatch command submits the job to the HPC scheduler with the running options specified in the header of the job script.\n\n\nsrc/steps/11_CheckLULCC/slurm_job.sh (lines 1-10)\n\n#!/bin/bash\n#SBATCH --job-name=\"11_check_lulcc\"\n#SBATCH -n 1                  # Number of cores requested\n#SBATCH --cpus-per-task=25    # Number of CPUs per task\n#SBATCH --time=4:00:00        # Runtime\n#SBATCH --mem-per-cpu=4G      # Memory per cpu in GB (see also --mem)\n#SBATCH --tmp=2G              # https://scicomp.ethz.ch/wiki/Using_local_scratch\n#SBATCH --output=\"logs/11_check_lulcc-%j.out\"\n#SBATCH --error=\"logs/11_check_lulcc-%j.err\"\n#SBATCH --mail-type=NONE       # Mail events (NONE, BEGIN, END, FAIL, ALL)\n\nChange these settings according to your needs and the available resources. Monitor the logs in the logs directory to check the progress of the job. If you want to specify more options, refer to the SLURM documentation or your local HPC documentation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setup and Usage</span>"
    ]
  },
  {
    "objectID": "pipeline/pipeline.html#20_FocalLULC",
    "href": "pipeline/pipeline.html#20_FocalLULC",
    "title": "3  Setup and Usage",
    "section": "3.5 Focal LULC",
    "text": "3.5 Focal LULC\nThis step calculates focal statistics for the land use and land cover change (LULCC) data. The resulting focal windows are used for the N-SDM model (Black 2024). It uses a similar structure to the previous Check LULCC step, as it uses another conda environment and this task also has a separate job script. The configuration section for this step is as follows:\n\n\nsrc/config.yml\n\n# Focal LULC\nFocalLULCC:\n  InputDir: # keep empty to use FUTURE_EI_OUTPUT_DIR/LULCC_CH_OUTPUT_BASE_DIR\n  OutputDir: # keep empty to use FUTURE_EI_OUTPUT_DIR/FOCAL_OUTPUT_BASE_DIR\n  BaseName: ch_lulc_agg11_future_pixel  # Underscores will be split into folders\n  RadiusList: [ 100, 200, 500, 1500, 3000 ]\n  WindowType: circle\n  FocalFunction: mean\n  Overwrite: False # False -&gt; skip if output exists, True -&gt; overwrite\n  Parallel: True\n  NWorkers: 0  # 0 means use all available cores\n\nThis script recursively goes through the input directory and calculates the focal statistics for each scenario. It creates the outputs in a similar structure, inside the output directory, named after the BaseName. For each scenario, the focal statistics by WindowType and FocalFunction are calculated for each radius in RadiusList. For details, consult the docstring of the method 20_focal_statistics::simulated_lulc_to_predictors.\nThis step uses a conda environment with raster~=3.6-26, terra~=1.7-71 (only used for conversion), and further R packages. The conda environment focal_lulc is set up by executing the setup script\nsrc/steps/20_FocalLULC/20_FocalLULC_setup.sh.\nAs for the previous steps, the job script slurm_job.sh needs to be submitted to the HPC scheduler.\nsbatch src/steps/20_FocalLULC/slurm_job.sh\n\n\n\n\n\n\nFocal LULC output check\n\n\n\nThe src/steps/20_FocalLULC/show_files.py script checks whether the focal window output files are complete. It verifies the presence of expected files in the output directory structure, calculates the percentage of completed files for each year, and lists any missing files. The script outputs a summary table and saves the missing file names to a text file called missing_files.txt.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setup and Usage</span>"
    ]
  },
  {
    "objectID": "pipeline/pipeline.html#40_NCPs",
    "href": "pipeline/pipeline.html#40_NCPs",
    "title": "3  Setup and Usage",
    "section": "3.6 Nature’s Contributions to People",
    "text": "3.6 Nature’s Contributions to People\nBased on the code written for Külling et al. (2024), we automatized the calculation of eight NCP. To note, our study includes more NCP as this, as some of them are characterized by the plain focal windows (Black et al. 2025). (Ben: true?)\nAdditionally to R and CRAN packages, InVEST is used via the Python module natcap.invest in this step.\n\n\n\n\n\n\nCaution 3.4: Used Versions in this step\n\n\n\nDue to previous API changes, the code is compatible with natcap.invest=3.13.0, but not earlier versions. raster=3.4-13 and terra=1.5-21 are used for the NCP calculation, among other packages6.\n\n\nAs the previous two steps, setting up the conda environment ncps is done using the src/steps/40_NCPs/40_NCPs_setup.sh script.\n\n3.6.1 NCPs\nTable 2.1 lists all NCP calculated in the evoland-plus HPC project. Here, we detail the eight NCP calculated in this step. The config.yml file includes a few variables which are automatically used for the NCP calculation.\n\n\nsrc/config.yml (54-59)\n\n  # NCP variables\n  NCP_PARAMS_YML: ~/evoland-plus HPC/src/steps/40_NCPs/NCP_models/40_NCPs_params.yml\n  NCP_RUN_SCENARIO_ID: # Scenario ID, automatically set for each configuration\n  NCP_RUN_YEAR: # Year for which to run NCPs, automatically set\n  NCP_RUN_OUTPUT_DIR: # Output directory for NCPs, automatically set\n  NCP_RUN_SCRATCH_DIR: # Scratch directory for NCPs, automatically set\n\nThe more detailed configuration for each NCP is stored in the 40_NCPs_params.yml file. For parallelization purposes, each array job receives a copy of this file with the respective scenario ID and year. The bash variables NCP_RUN_* from the config.yml act as a placeholder.\n\n\nsrc/steps/40_NCPs/NCP_models/40_NCPs_params.yml\n\n# Run Params (are passed when calling the run_all_ncps.py script)\nrun_params:\n  NCP_RUN_SCENARIO_ID:\n  NCP_RUN_YEAR:\n  NCP_RUN_RCP:  # programmatically set in load_params.py\n  NCP_RUN_INPUT_DIR:\n  NCP_RUN_OUTPUT_DIR:\n  NCP_RUN_SCRATCH_DIR:\n  LULCC_M_EI_LAYER_DIR:  # set in load_params.py (uses config.yml)  # SDR\n\nFor preparation, it is indispensable to set the paths to the input data. Some of these are shared among multiple NCP, as noted in the comments. The first three layers are automatically found in the NCP_RUN_INPUT_DIR and depend on the scenario ID and year. These three are constructed with the template that the LULCC model produces, as can be seen in the load_params.py script.\n\n\nsrc/steps/40_NCPs/NCP_models/40_NCPs_params.yml\n\n# Data\ndata:\n  # LULC               - CAR, FF, HAB, NDR, POL, SDR, WY\n  lulc: # automatically found in NCP_RUN_INPUT_DIR\n  # Rural residential  - HAB\n  rur_res: # automatically found in NCP_RUN_INPUT_DIR\n  # Urban residential  - HAB\n  urb_res: # automatically found in NCP_RUN_INPUT_DIR\n  # Production regions - CAR\n  prodreg: Data/PRODUCTION_REGIONS/PRODREG.shp\n  # DEM                - CAR, NDR\n  dem: Data/DEM_mean_LV95.tif\n  # DEM filled         - SDR\n  dem_filled: Data/DEM_mean_LV95_filled.tif\n  # Wathersheds        - NDR, SDR, WY\n  watersheds: Data/watersheds/watersheds.shp\n  # Subwatersheds      - WY\n  sub_watersheds: Data/watersheds/Subwatersheds.shp\n  # ETO                - WY\n  eto: Data/evapotranspiration/\n  # PAWC               - WY\n  pawc: Data/Water_storage_capacity_100m_reclassified1.tif\n  # Erodibility path   - SDR\n  erodibility_path: Data/Kst_LV95_ch_nib.tif\n  # Erosivity path     - SDR\n  erosivity_path: Data/rainfall_erosivity/\n  # Precipitation      - WY, NDR\n  yearly_precipitation: Data/yearly_prec/\n  # Soil depth         - WY\n  depth_to_root_rest_layer: Data/rrd_100_mm_rexport.tif\n  # Precipitation avgs - FF\n  pavg_dir: Data/monthly_prec/\n  # Temperature avgs   - FF\n  tavg_dir: Data/monthly_temp/\n  # Soil texture       - FF\n  ph_raster: Data/ch_edaphic_eiv_descombes_pixel_r.tif\n  # Distance to lakes  - REC\n  distlakes_path: Data/distlakes.tif\n\n# Projection Settings - change for different regions\nproj:\n  # CRS\n  crs: epsg:2056\n  # Extent\n  ext: [ 2480000, 2840000, 1070000, 1300000 ]\n  # Resolution\n  res: 100\n\n\n\n\n\n\n\nCaution 3.5: Resolution\n\n\n\nMake sure that the resolution of all input data is the same and matches the proj.res setting in the 40_NCPs_params.yml file.\n\n\nFor each NCP, the configuration is detailed in the following sections.\n\n3.6.1.1 CAR: Regulation of climate\n\n\nsrc/steps/40_NCPs/NCP_models/40_NCPs_params.yml\n\nCAR:\n  # 1_CAR_S_CH.R\n  # 2_CAR_S_CH.py\n  bp_tables_dir:\n    evoland-plus HPC/src/steps/40_NCPs/NCP_models/CAR/BPTABLE/\n  # 3_CAR_S_CH.R\n  # output prefix\n  out_prefix: tot_c_cur_\n\nTo calculate the carbon stored in biomass and soil, the CAR NCP needs biophysical tables that specify the carbon content of different land use classes. The natcap.invest-model Carbon Storage and Sequestration is used for this calculation.\n\n\n3.6.1.2 FF: Food and feed\n\n\nsrc/steps/40_NCPs/NCP_models/40_NCPs_params.yml\n\nFF:\n  # 0_FF_ecocrop.R\n  crops_data:\n    evoland-plus HPC/src/steps/40_NCPs/NCP_models/FF/crops.txt\n  ecocrop_dir: evoland-plus HPC-Output/FF_preprocessing_ecocrop/\n\nThe FF NCP calculates the crop production potential using the ecocrop package. The package uses a limiting factor approach Hackett (1991). This NCP has a data preparation step which needs to be executed once before running the parallelized NCP calculation. It is a single R script that can easily be triggered with calling src/steps/40_NCPs/NCP_models/prepare_ncps.sh, no SLURM needed.\n\n\n3.6.1.3 HAB: Habitat creation and maintenance\n\n\nsrc/steps/40_NCPs/NCP_models/40_NCPs_params.yml\n\nHAB:\n  # 0_thread_layers_generation.R\n  # 1_HAB_S_CH.py\n  half_saturation_constant: 0.075\n  bp_table_path:\n    evoland-plus HPC/src/steps/40_NCPs/NCP_models/HAB/BPTABLE/\n  sensitivity_table_path:\n    evoland-plus HPC/src/steps/40_NCPs/NCP_models/HAB/BPTABLE/hab_sensitivity.csv\n  threats_table_path:\n    evoland-plus HPC/src/steps/40_NCPs/NCP_models/HAB/BPTABLE/threats.csv\n\nThe HAB NCP calculates the habitat quality index, another natcap.invest model. Set the three biophysical tables accordingly.\nAs we had problems how natcap.invest==3.13.0 handles its treat layer table, we had to introduce a hotfix in the source code to keep compatibility with the existing NCP configuration. When loading in the threat layers, natcap.invest wants to convert the column names to lowercase to be case-insensitive, but the layer paths are also converted to lowercase, but our threat layers are case-sensitive. To fix this bug, we changed the to_lower argument in the execute function in the habitat_quality.py file and set the column name to match our lowercase column name.\n\n\n.../ncps/lib/python3.10/site-packages/natcap/invest/habitat_quality.py (line 384)\n\n# Change from:\n            args['threats_table_path'], 'THREAT', to_lower=True,\n# to:\n            args['threats_table_path'], 'threat', to_lower=False,\n\nIn later versions, the InVEST developers have changed the modality of loading in these tables. Compatibility with the latest version of natcap.invest can be added when adapting breaking changes with the further NPC. We want to note that changing the source code is a bad practice and should only be considered as a last resort.\nTo find the corresponding natcap folder, navigate to the environment folder, from where you find the site-packages folder.\n# activate the ncps environment with micromamba or conda\nmicromamba activate ncps\n# find the site-packages folder\npython -c \"import site; print(site.getsitepackages())\"\n&gt;&gt;&gt; ['.../micromamba/envs/ncps/lib/python3.10/site-packages']\nIn this folder, you navigate further down to find .../site-packages/natcap/invest/habitat_quality.py.\n\n\n3.6.1.4 NDR: Nutrient Delivery Ratio\n\n\nsrc/steps/40_NCPs/NCP_models/40_NCPs_params.yml\n\nNDR:\n  # 1_NDR_S_CH.py\n  # Biophysical table\n  biophysical_table_path:\n    evoland-plus HPC/src/steps/40_NCPs/NCP_models/NDR/BPTABLE/ndr_bptable_ds25_futei.csv\n  calc_n: true\n  calc_p: true\n  k_param: 2\n  # Suffix for output files\n  # Subsurface critical length\n  subsurface_critical_length_n: 100\n  # Subsurface effective retention\n  subsurface_eff_n: 0.75\n  # Threshold flow accumulation\n  threshold_flow_accumulation: 200\n\nThe NDR NCP calculates the Nutrient Delivery Ratio. The biophysical table specifies the nutrient retention by vegetation using various variables, e.g., root depth and more detailed soil properties described in the natcap.invest documentation.\n\n\n3.6.1.5 POL: Pollination and dispersal of seeds\n\n\nsrc/steps/40_NCPs/NCP_models/40_NCPs_params.yml\n\nPOL:\n  # 1_POL_S_CH.py\n  # Farm vector path\n  farm_vector_path: ''\n  # Guild table path\n  guild_table_path:\n    evoland-plus HPC/src/steps/40_NCPs/NCP_models/POL/BPTABLE/guild.csv\n  # Landcover biophysical table path\n  landcover_biophysical_table_path:\n    evoland-plus HPC/src/steps/40_NCPs/NCP_models/POL/BPTABLE/pollination_bptable_ds25_futei.csv\n  # 2_POL_S_CH_aggregating.R\n\nThe POL NCP calculates the natcap.invest Crop Pollination model. Followed by an aggregation step in R.\n\n\n3.6.1.6 REC: Recreation potential\n\n\nsrc/steps/40_NCPs/NCP_models/40_NCPs_params.yml\n\nREC:\n  # 1_REC.R\n  # lulc naturality lookup table\n  lutable_nat_path:\n    evoland-plus HPC/src/steps/40_NCPs/NCP_models/REC/BPTABLE/lutable_naturality.csv\n\nThe REC NCP returns a Recreation Potential (RP) indicator. This is a normalized aggregate of three landscape characteristics maps:\n\nDegree of naturalness (DN): Aggregate sum of naturalness scores for each LULC class.\nNatural protected areas (NP): Binary map of 0=outside protected areas, 1=inside protected areas.\nWater components (W): Inverse relative distance to lake coasts, with the highest value at the lake coast and a decreasing value for 2 km.\n\nThe output is a single map of recreation potential.\n\n\n3.6.1.7 SDR: Formation, protection and decontamination of soils\n\n\nsrc/steps/40_NCPs/NCP_models/40_NCPs_params.yml\n\nSDR:\n  # 1_SDR_S_CH.py\n  # Biophysical table\n  biophysical_table_path:\n    evoland-plus HPC/src/steps/40_NCPs/NCP_models/SDR/BPTABLE/bptable_SDR_v2_futei.csv\n  # Drainage path\n  ic_0_param: 0.4\n  k_param: 2\n  l_max: 100\n  # SDR max\n  sdr_max: 0.75\n  # Threshold flow accumulation\n  threshold_flow_accumulation: 200\n\nSediment export and retention are calculated in the SDR NCP with the Sediment Delivery Ratio model from natcap.invest.\n\n\n3.6.1.8 WY: Regulation of freshwater quantity, location and timing\n\n\nsrc/steps/40_NCPs/NCP_models/40_NCPs_params.yml\n\nWY:\n  # 1_WY_S_CH.py\n  # Biophysical table\n  biophysical_table_path:\n    evoland-plus HPC/src/steps/40_NCPs/NCP_models/WY/BPTABLE/wy_bptable_ds25_futei.csv\n  # Seasonality constant\n  seasonality_constant: 25\n\nAnnual Water Yield is the final NCP calculated in this step. The WY NCP calculates the hydropower potential.\n\n\n\n3.6.2 Running the NCP calculation\nAssuming the ncps environment is set up, all previous configurations are correctly set, the input data is available, and the FF NCP has been prepared using src/steps/40_NCPs/NCP_models/prepare_ncps.sh, the NCP calculation can be started.\nTo calculate all NCP for one scenario and year, the run_all_ncps.py script bundles the execution of all NCP. It is used like so:\n# Usage: bash run_all_ncps.sh &lt;NCP_RUN_SCENARIO_ID&gt; &lt;NCP_RUN_YEAR&gt; &lt;NCP_RUN_INPUT_DIR&gt; &lt;NCP_RUN_OUTPUT_DIR&gt; &lt;NCP_RUN_SCRATCH_DIR&gt;\nbash src/steps/40_NCPs/NCP_models/run_all_ncps.sh 1 2015 /path/to/input_dir /path/to/output_dir /path/to/scratch_dir\nThe simplified execution of this using the HPC scheduler SLURM is done with sbatch src/steps/40_NCPs/NCP_models/slurm_job.sh. The scenario ID and year are set in the job script.\nThe full, parallelized execution of the evoland-plus HPC pipeline for all scenarios with LULCC and NCP calculation is done with the 10_40_combined_array_job.sh script and SLURM, for this consult the following Running section.\n\n\n\n\n\n\nNCP output check\n\n\n\nThe src/steps/40_NCPs/show_files.py script shows existing NCP results by scenario. It reads numbers from files to generate a histogram of counts and checks if each expected file is present in each scenario. The script outputs a summary table of file coverage and lists any missing or unexpected files. Missing files are saved to scenarios_with_missing_files.txt, and unexpected files are saved to unexpected_files.txt. Such unexpected files might be intermediate files that are not cleaned up properly and can be deleted.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setup and Usage</span>"
    ]
  },
  {
    "objectID": "pipeline/pipeline.html#running",
    "href": "pipeline/pipeline.html#running",
    "title": "3  Setup and Usage",
    "section": "3.7 Running",
    "text": "3.7 Running\nThe pipeline is executed in three parts, each part is a separate Slurm job. Remember Figure 2.1 from the Structure section. The most computationally intensive steps, LULCC and NCP are parallelized and submitted as one Slurm array job. For all of these steps, you need to have followed the previous sections to set up and configure the pipeline. This includes preparing the FF NCP using src/steps/40_NCPs/NCP_models/prepare_ncps.sh and filling the simulation control table with all the scenarios you want to run.\n\n3.7.1 evoland-plus HPC pipeline\nLand Use Simulation and NCP Estimation can separately be calculated for one scenario with the jobs src/steps/10_LULCC/slurm_job.sh and src/steps/40_NCPs/slurm_job.sh. The 10_40_combined_array_job.sh slurm job calculates both steps for all scenarios in parallel. Each array job receives a subset of the scenarios to calculate. All scenarios are calculated in parallel with the following slurm job:\nsbatch src/steps/10_40_combined_array_job.sh\nThis would submit the job to the cluster and start the calculation with the default settings.\n\n\nsrc/steps/10_40_combined_array_job.sh\n\n#!/bin/bash\n#SBATCH --job-name=\"10_40_combined_array\"\n#SBATCH -n 1                  # Number of cores requested\n#SBATCH --cpus-per-task=2     # Number of CPUs per task\n#SBATCH --time=7-00:00:00     # Runtime in D-HH:MM:SS\n#SBATCH --mem-per-cpu=2G\n#SBATCH --tmp=2G\n#SBATCH --output=\"logs/10_40_combined_array-%j.out\"\n#SBATCH --error=\"logs/10_40_combined_array-%j.err\"\n#SBATCH --mail-type=NONE      # Mail events (NONE, BEGIN, END, FAIL, ALL)\n## Array job\n#SBATCH --array=1-216%12      # start-end%num_parallel\n#        ! step size needs to be 1\n\nThe speed-up of the combined job is achieved by running multiple scenarios in parallel. We do this, as the speed-up assigning more CPUs to one scenario is limited. Each of the 216 array jobs is assigned one core with two CPUs and 4 GB of memory. %12 in the array specification ensures that 12 array jobs are run in parallel, if one job finishes, the next one is started. Each array job has a time limit of 7 days.\nIn our case, we had 1080 scenarios to calculate, so we set the array job to run 216 scenarios in parallel to have five scenarios per array job. Before, we have tested with only having one scenario in the simulation control table, 10 GB of memory, and SBATCH --array=1-1 to check if the job runs correctly. Running the Switzerland map at a resolution of 100 m by 100 m, the job took 6:23:06 hours to complete at a CPU efficiency of 75.29% and a memory efficiency of 75.58%. With tail -f logs/10_40_combined_array_-*.out logs/10_40_combined_array_-*.err it is easy to monitor the progress of the job. For explanations and more details on the sbatch options, see the Slurm documentation.\nWhen running a large array of scenarios, the array jobs vary in the amount of memory they require and time they take. It is a valid approach to start with a memory limit that works for the majority of scenarios. Some jobs might fail due to memory issues, but after all array jobs have finished, it is possible to rerun the failed scenarios with a higher memory limit. This is possible because the LULCC and NCP are only calculated if each output file is missing, down to the level of each NCP.\n\n\n\n\n\n\nThe cluster might have a limit on the number of array jobs that can be run in parallel. To find out the limit, use scontrol show config | grep MaxArraySize.\n\n\n\nTo get a simple estimation on how long the job array takes, you can use cross-multiplication, starting with the time it took to calculate one scenario \\(t_{\\text{one}}\\). With the number of scenarios \\(n_{\\text{all}}\\) and the number of scenarios calculated in parallel \\(n_{\\text{parallel}}\\), the time it takes to calculate all scenarios \\(t_{\\text{all}}\\) is:\n\\[\nt_{\\text{all}} = \\frac{n_{\\text{all}}}{n_{\\text{parallel}}} \\times t_{\\text{one}}\n\\]\n\n\n\n\n\n\nTip 3.1: Selective running\n\n\n\nIf you only want to either run the LULCC or the NCP, you can modify the 10_40_combined_array_job.sh script to only run the respective part. This comes down to commenting-out one line in the script.\n\n\n\n\n3.7.2 Check LULCC and Focal LULC\nAs explained in their respective sections, the steps Check LULCC and Focal LULC can be already run after the LULC layers are present. Both src/steps/11_CheckLULCC/slurm_job.sh and src/steps/20_FocalLULC/slurm_job.sh are also submitted with sbatch. In contrast, these are simple jobs and their parallelization is achieved by assigning more CPUs to the job and using R’s asynchronous processing future::plan(future::multisession).\n\n\n3.7.3 Logging\nThere are multiple levels of logging in the pipeline. When running the Slurm jobs, the output and error logs are written to the specified files. These are coming from three main sources: the R scripts, the Python scripts, and the Slurm job scripts. Generally, slurm logs are written to the file specified in the job script. For the logs regarding the scripts written for this pipeline, FUTURE_EI_LOG_LEVEL: debug in the config.yml file can be set to debug, info, warning, or error. The NCP calculation uses natcap.invest which has detailed logs written to the console. For the LULCC container, Dinamica EGO has more detailed logs of the integrated R scripts. They are written to the mounted LULCC_CH_HPC_DIR directory and do not show up in the Slurm logs. Dinamica EGO has a separate log level that can be set through the DINAMICA_EGO_CLI_LOG_LEVEL environment variable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setup and Usage</span>"
    ]
  },
  {
    "objectID": "pipeline/pipeline.html#footnotes",
    "href": "pipeline/pipeline.html#footnotes",
    "title": "3  Setup and Usage",
    "section": "",
    "text": "The installed CLI is identified via bash variables in src/bash_common.sh. If none is found, an error highlights the issue.↩︎\nThe Docker version used is 24.0.7, but the container should be compatible with most versions.↩︎\nWe have used yq v4.40.3, but any version &gt;=4.18.1 should work.↩︎\nThe versions of the R packages used with LULCC are listed in the note below.\n\n\n\n\n\n\nR Packages\n\n\n\nVersions used with LULCC docker 0.3.0:\n\nraster: 3.6-26\ntidyverse: 2.0.0\ndata.table: 1.15.4\nrandomForest: 4.7-1.1\ncallr: 3.7.6\nfuture: 1.33.2\nfuture.apply: 1.11.2\nfuture.callr: 0.8.2\nsp: 2.1-3\nstringi: 1.8.3\nstringr: 1.5.1\n\n\n\n↩︎\nFor more information on the compatibility of Dinamica EGO with Linux, see the Dinamica EGO documentation.↩︎\nThe versions of the packages used in the NCP calculation are listed in the note below.\n\n\n\n\n\n\nVersions used for the NCP calculation\n\n\n\nR (4.1.3) packages:\n\nraster: 3.4-13\nterra: 1.5-21\nmeteor: 0.4.5\nRecocrop: 0.4.0\nrgdal: 1.5-29\ncodetools: 0.2-19\ndata.table: 1.14.8\nremotes: 2.4.2\nsf: 1.0-7\nyaml: 2.3.7\n\nPython (3.10.13) packages:\n\nnatcap.invest: 3.13.0\n\n\n\n↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setup and Usage</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "4  Summary",
    "section": "",
    "text": "The evoland-plus HPC pipeline provides a comprehensive framework for exploring future ecosystem services and nature’s contributions to people (NCP) in the context of climate scenarios. By following the steps outlined in this guide, users can generate and analyze LULC layers, focal windows, and NCP, and extend their analysis with additional steps.\nKey points:\n\nOverview of the evoland-plus HPC pipeline and its components\nDetailed steps for setting up and configuring the pipeline\nGeneration of LULC layers, focal windows, and NCP\nIntegration of additional steps for more comprehensive analysis (?)\nApplications (?)\n\nResults and implications …\nBullet points for your notes to elaborate on regarding the results in your paper:\n\nSummarize the main findings of the evoland-plus HPC pipeline analysis\nDiscuss the implications of the results for biodiversity conservation\nHighlight the impact of different climate scenarios on ecosystem services\nExplain the significance of LULC changes in the context of the study\nProvide examples of how the results can inform policy and management decisions\nMention any limitations of the study and potential areas for future research",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Adde, Antoine, Pierre-Louis Rey, Philipp Brun, Nathan Külling, Fabian\nFopp, Florian Altermatt, Olivier Broennimann, et al. 2023.\n“N-SDM : A\nHigh-Performance Computing Pipeline for Nested Species\nDistribution Modelling.” Ecography 2023 (6):\ne06540. https://doi.org/10.1111/ecog.06540.\n\n\nBlack, Benjamin. 2024. “LULCC-CH Source Code.” GitHub. https://github.com/blenback/LULCC-CH.\n\n\nBlack, Benjamin, Antoine Adde, Daniel Farinotti, Antoine Guisan, Nathan\nKülling, Manuel Kurmann, Caroline Martin, et al. 2024. “Broadening\nthe Horizon in Land Use Change Modelling: Normative Scenarios for Nature\nPositive Futures in Switzerland.” Regional Environmental\nChange 24 (3). https://doi.org/10.1007/s10113-024-02261-0.\n\n\nBlack, Benjamin, Antoine Adde, Nathan Külling, Carlson Büth, Manuel\nKurmann, Anthony Lehmann, Florian Altermatt, Antoine Guisan, and\nAdrienne Gret-Regamey. 2025. “Identifying Robust Conservation\nStrategies to Secure Ecosystem Service Provision Under\nUncertainties.”\n\n\nBlack, Benjamin, Maarten J. Van Strien, Antoine Adde, and Adrienne\nGrêt-Regamey. 2023. “Re-Considering the Status Quo:\nImproving Calibration of Land Use Change Models Through\nValidation of Transition Potential Predictions.”\nEnvironmental Modelling & Software 159 (January): 105574.\nhttps://doi.org/10.1016/j.envsoft.2022.105574.\n\n\nHackett, C. 1991. “Mobilising Environmental Information about\nLesser-Known Plants: The Value of Two Neglected Levels of\nDescription.” Agroforestry Systems 14 (2): 131–43. https://doi.org/10.1007/bf00045728.\n\n\nKülling, Nathan, Antoine Adde, Audrey Lambiel, Sergio Wicki, Antoine\nGuisan, Adrienne Grêt-Regamey, and Anthony Lehmann. 2024.\n“Nature’s Contributions to People and Biodiversity Mapping in\nSwitzerland: Spatial Patterns and Environmental Drivers.”\nEcological Indicators 163: 112079. https://doi.org/10.1016/j.ecolind.2024.112079.\n\n\nLeite-Filho, Argemiro T., Britaldo S. Soares-Filho, Juliana L. Davis,\nand Hermann O. Rodrigues. 2020. Modeling Environmental Dynamics with\nDinamica EGO. Centro de Sensoriamento Remoto. Universidade Federal\nde Minas Gerais, Belo Horizonte, Minas Gerais. https://www.csr.ufmg.br/dinamica/dokuwiki/doku.php?id=guidebook_start.\n\n\nMayer, Paula, Sven-Erik Rabe, and Adrienne Grêt-Regamey. 2023.\n“Operationalizing the Nature Futures Framework for\nEcological Infrastructure.” Sustainability Science,\nJuly. https://doi.org/10.1007/s11625-023-01380-7.\n\n\nR Core Team. 2022. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.",
    "crumbs": [
      "References"
    ]
  }
]